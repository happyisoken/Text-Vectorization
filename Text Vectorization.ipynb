{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0cfa4fd-e3fd-4585-86c7-3364ac544da2",
   "metadata": {},
   "source": [
    "#### Text Vectorization\n",
    "\n",
    "Text Vectorization is the process of converting text into numerical representation. \n",
    "\n",
    "Here are some popular methods to accomplish text vectorization:\n",
    "\n",
    "1. Binary Term Frequency: captures presence (1) or absence (0) of term in document.\n",
    "\n",
    "2. Bag of Words (BoW) Term Frequency: captures frequency of term in document. \n",
    "\n",
    "3. (L1) Normalized Term Frequency: captures normalized BoW term frequency in document.\n",
    "\n",
    "4. (L2) Normalized TF-IDF(Term Frequency–Inverse Document Frequency): captures normalized TFIDF in document.\n",
    "\n",
    "5. Word2Vec: provides embedded representation of words. Word2Vec starts with one representation of all words in the corpus and train a NN (with 1 hidden layer) on a very large corpus of data. \n",
    "\n",
    "Here are the two methods that is typically used for training the NN:\n",
    "• Continuous Bag of Words (CBOW) — Predict vector representation of center/target word based on window of context words.\n",
    "• Skip-Gram (SG) — Predict vector representation of window of context words based on center/target word \n",
    "\n",
    "#### Case Study: Book Recommendations from Charles Darwin\n",
    "\n",
    "Data\n",
    "\n",
    "Charles Darwin is the most famous scientist in the world. He wrote many other books on a wide range of topics, including geology, plants or his personal life. In this project, we will develop a content-based book recommendation system, which will determine which books are close to each other based on how similar the discussed topics are. \n",
    "\n",
    "##### Text Preprocessing\n",
    "\n",
    "As the first step, we need to load the content of each book and check the regular expression to facilitate the process by removing the all non-alpha-numeric characters. We call such a collection of texts a CORPUS.\n",
    "\n",
    "Next step, we transform the corpus into a format by doing tokenization.\n",
    "\n",
    "For the next parts of text preprocessing, we use a stemming process, which will group together the inflected forms of a word so they can be analyzed as a single item: the stem. In order to make the process faster, we will directly load the final results from a pickle file and review the method used to generate it.\n",
    "\n",
    "#### Text Vectorization\n",
    "\n",
    "##### Bag-of-Words Models (BoW)\n",
    "\n",
    "First, we need to create a universe of all words contained in our corpus of Charles Darwin’s books, which we call a dictionary.Then, using the stemmed tokens and the dictionary, we will create bag-of-words models (BoW) to represent our books as a list of all unique tokens they contain associated with their respective number of occurrences. \n",
    "\n",
    "##### TF-IDF Model\n",
    "\n",
    "Next, we will use a TF-IDF model to define the importance of each word depending on how frequent it is in the text. As a result, a high TF-IDF score for a word will indicate that this word is specific to this text.\n",
    "\n",
    "##### Recommendation\n",
    "\n",
    "Now that we have a TF-IDF model on how specific they are to each book, we can measure how related to books are between each other. Therefore, we will use Cosine Similarity and visualize the results as a distance matrix.\n",
    "\n",
    "##### Conclusion\n",
    "\n",
    "We now have a matrix containing all the similarity measures between any pair of books from Charles Darwin! We can use barh() to display a horizontal bar plot for which books are the most similar to “On the Origin of Species.”\n",
    "\n",
    "However, we want to have a better understanding of the big picture and see how Darwin’s books are generally related to each other. To this purpose, we will represent the whole similarity matrix as a dendrogram, which is a standard tool to display such data.\n",
    "\n",
    "Finally, based on the chart we created before, we can conclude that “the variation of animals and plants under domestication” is most related to “On the Origin of Species.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ddb75-ed82-4714-8a78-e02dc8e611a2",
   "metadata": {},
   "source": [
    "#### Machine Learning Text Processing and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32bfaae6-3946-4dfc-9778-96c8d1002aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset - training data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset = 'train', shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4401d0-0cbb-499c-94ef-8f6704f86277",
   "metadata": {},
   "source": [
    "Types of data:\n",
    "1. Structured data: Data in numeric form.E.g. csv file\n",
    "2. Unstructured data: Not in numeric form.\n",
    "\n",
    "##### Vectorization:\n",
    "\n",
    "The process of converting structured to unstructure data is called vectorization. Vectorization is the process of converting text/unstr (human readable language) into meaningful numeric/vectors representation.\n",
    "\n",
    "We cannot work with text directly when using Machine learning algorithms. Instead, we need to convert the text to numbers.\n",
    "\n",
    "The mothods used are: \n",
    "\n",
    ". Bag of words(BOW) model: A simple and effective model for thinking about text documents in ML. This can be done by assigning each word a unique number.\n",
    "\n",
    ". Count Vectorizer: Provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n",
    "\n",
    ". TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b4d743-73cb-4524-8504-68f0b2a84ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of using the CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19a8df47-a7f1-4b2e-9b49-7b15063f4325",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['The quick brown fox jumped over the lazy dog.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a8d005a-dda9-4479-93b2-b23e15ab02b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox jumped over the lazy dog.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a0bfa1-856f-4f53-8b6e-9a4fffc18db1",
   "metadata": {},
   "source": [
    "Tokenization refers to splitting up a larger body of text into smaller lines, words, or even creating words for a non-English language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23871562-97a9-49b4-b443-3fa06cc8252b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "905e01b8-f67a-47f8-b47e-73a7e84809cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.target) # number of target variables(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "407e01a6-5546-4d7a-b6e4-648b6a6458c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4, 4, ..., 3, 1, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da3b2536-9a65-425b-a2ce-c9051d09e8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02fe6716-1cce-48c4-9c23-17c56843c140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.data) # number of input records/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eff38bb-1828-460b-8701-c8e012ae7c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\",\n",
       " \"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\",\n",
       " 'From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\\nSubject: PB questions...\\nOrganization: Purdue University Engineering Computer Network\\nDistribution: usa\\nLines: 36\\n\\nwell folks, my mac plus finally gave up the ghost this weekend after\\nstarting life as a 512k way back in 1985.  sooo, i\\'m in the market for a\\nnew machine a bit sooner than i intended to be...\\n\\ni\\'m looking into picking up a powerbook 160 or maybe 180 and have a bunch\\nof questions that (hopefully) somebody can answer:\\n\\n* does anybody know any dirt on when the next round of powerbook\\nintroductions are expected?  i\\'d heard the 185c was supposed to make an\\nappearence \"this summer\" but haven\\'t heard anymore on it - and since i\\ndon\\'t have access to macleak, i was wondering if anybody out there had\\nmore info...\\n\\n* has anybody heard rumors about price drops to the powerbook line like the\\nones the duo\\'s just went through recently?\\n\\n* what\\'s the impression of the display on the 180?  i could probably swing\\na 180 if i got the 80Mb disk rather than the 120, but i don\\'t really have\\na feel for how much \"better\" the display is (yea, it looks great in the\\nstore, but is that all \"wow\" or is it really that good?).  could i solicit\\nsome opinions of people who use the 160 and 180 day-to-day on if its worth\\ntaking the disk size and money hit to get the active display?  (i realize\\nthis is a real subjective question, but i\\'ve only played around with the\\nmachines in a computer store breifly and figured the opinions of somebody\\nwho actually uses the machine daily might prove helpful).\\n\\n* how well does hellcats perform?  ;)\\n\\nthanks a bunch in advance for any info - if you could email, i\\'ll post a\\nsummary (news reading time is at a premium with finals just around the\\ncorner... :( )\\n--\\nTom Willis  \\\\  twillis@ecn.purdue.edu    \\\\    Purdue Electrical Engineering\\n---------------------------------------------------------------------------\\n\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\\nNietzsche\\n',\n",
       " 'From: jgreen@amber (Joe Green)\\nSubject: Re: Weitek P9000 ?\\nOrganization: Harris Computer Systems Division\\nLines: 14\\nDistribution: world\\nNNTP-Posting-Host: amber.ssd.csd.harris.com\\nX-Newsreader: TIN [version 1.1 PL9]\\n\\nRobert J.C. Kyanko (rob@rjck.UUCP) wrote:\\n> abraxis@iastate.edu writes in article <abraxis.734340159@class1.iastate.edu>:\\n> > Anyone know about the Weitek P9000 graphics chip?\\n> As far as the low-level stuff goes, it looks pretty nice.  It\\'s got this\\n> quadrilateral fill command that requires just the four points.\\n\\nDo you have Weitek\\'s address/phone number?  I\\'d like to get some information\\nabout this chip.\\n\\n--\\nJoe Green\\t\\t\\t\\tHarris Corporation\\njgreen@csd.harris.com\\t\\t\\tComputer Systems Division\\n\"The only thing that really scares me is a person with no sense of humor.\"\\n\\t\\t\\t\\t\\t\\t-- Jonathan Winters\\n',\n",
       " 'From: jcm@head-cfa.harvard.edu (Jonathan McDowell)\\nSubject: Re: Shuttle Launch Question\\nOrganization: Smithsonian Astrophysical Observatory, Cambridge, MA,  USA\\nDistribution: sci\\nLines: 23\\n\\nFrom article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\\n>>In article <C5JLwx.4H9.1@cs.cmu.edu>, ETRAT@ttacs1.ttu.edu (Pack Rat) writes...\\n>>>\"Clear caution & warning memory.  Verify no unexpected\\n>>>errors. ...\".  I am wondering what an \"expected error\" might\\n>>>be.  Sorry if this is a really dumb question, but\\n> \\n> Parity errors in memory or previously known conditions that were waivered.\\n>    \"Yes that is an error, but we already knew about it\"\\n> I\\'d be curious as to what the real meaning of the quote is.\\n> \\n> tom\\n\\n\\nMy understanding is that the \\'expected errors\\' are basically\\nknown bugs in the warning system software - things are checked\\nthat don\\'t have the right values in yet because they aren\\'t\\nset till after launch, and suchlike. Rather than fix the code\\nand possibly introduce new bugs, they just tell the crew\\n\\'ok, if you see a warning no. 213 before liftoff, ignore it\\'.\\n\\n - Jonathan\\n\\n\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86ca95af-cef2-4a1d-9ad0-cfe8175ade7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25103659-f30a-44d3-8c49-a90088cbd021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of text documents\n",
    "text = ['The quick brown fox jumped over the lazy dog.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dc1d181-3197-4a7c-8e78-822bed1c531f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox jumped over the lazy dog.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8e5c4b9-2393-45de-9fcd-b91fad9fa486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "(1, 8)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6019407a-9f5f-476a-8445-5683dd01aa3e",
   "metadata": {},
   "source": [
    "CountVectorizer converts your text into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b043e17-3c66-4ed6-952d-a084bc63804c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 7,\n",
       " 'quick': 6,\n",
       " 'brown': 0,\n",
       " 'fox': 2,\n",
       " 'jumped': 3,\n",
       " 'over': 5,\n",
       " 'lazy': 4,\n",
       " 'dog': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a626732f-a1a4-43b3-9b90-4b2940b2732c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4e4a0dc-73f9-4694-955e-52374e4535ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Encode another document\n",
    "text2 = ['the puppy']\n",
    "vector = vectorizer.transform(text2)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b01e96dc-23e9-4486-99fd-74ad0b586720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Encode another document\n",
    "text2 = ['the brown puppy brown']\n",
    "vector = vectorizer.transform(text2)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bcd3aad-0db5-4086-b338-abbbe90057b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox jumped over the lazy dog.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf43c95-65a2-4539-8590-61505fdfb484",
   "metadata": {},
   "source": [
    "Working with text documents: preprocessing and vectorizing : different types\n",
    "\n",
    "Types of data:\n",
    "1. Structured: in numeric form.. csv file which has only numbers\n",
    "2. Unstructured: not in numeric form ..ex: images, text data.\n",
    "\n",
    "Vectorization: process of converting text/unstr (human readable lang) into meaningful numeric/vectors\n",
    "\n",
    "We cannot work with text directly when using ML algorithms. Istead, we need to convert the text to numbers.\n",
    "\n",
    "A bag of words(BOW: \n",
    "- a way of extracting features from text for use in modelling, such as with ML algorithms. It is simple and flexible to use.\n",
    "\n",
    "- It is a representation of text that describes the occurence of words within a document and it involves two things.\n",
    "\n",
    "- A simple and effective model for thinking about text documents in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94630df2-d331-46b3-985a-8614b9e04743",
   "metadata": {},
   "source": [
    "Vectorization:\n",
    "    1. bag of words model.\n",
    "    2. Count vectorizer.\n",
    "    3. TF-IDF vectorizer.\n",
    "    \n",
    "Word Counts with CountVectorizer: The CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also encode new documents using that vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7ecbcf6-18e5-4f4f-9235-a4d443547c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#list of text documents\n",
    "#create the transforn\n",
    "vectorizer = CountVectorizer()\n",
    "#tokenize and build vocab\n",
    "vector = vectorizer.fit_transform(text)\n",
    "#summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2efbcb3c-9320-4a0b-b1da-159e7a0ea69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce33448c-c289-4de7-943b-5788affca602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a44ede-d361-49a1-80ce-984af6b94fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Continue from 24:25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b25640a-167d-41b9-a906-e85cf6da1762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2651e-106b-435e-978e-017b886a09c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c465e20-47c6-41b0-b742-04247dd0d680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
